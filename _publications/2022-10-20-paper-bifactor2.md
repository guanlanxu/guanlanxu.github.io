---
title: "Expanding Bifactor Models of Psychological Traits to Account for Multiple Sources of Measurement Error"
collection: publications
permalink: /publication/2022-10-20-paper-bifactor2
excerpt: "We demonstrate how multi-occasion bifactor models can be used to gauge effects of multiple sources of measurement error, revise measures to reduce such error, improve model fit, assess dimensionality and viability of scale scores, and interpret results from both factor analytic and generalizability theory perspectives. "
date: 2022-10-20
venue: "Psychological Assessment"
citation: 'Vispoel, W. P., Lee, H., Xu, G., & Hong, H. (2022). Expanding bifactor models of psychological traits to account for multiple sources of measurement error. Psychological Assessment.'
---

Abstract

Over the last decade, applications of bifactor modeling within clinical settings have increased markedly but typically rely on data collected on single occasions. A shortcoming of such research is that reliability coefficients are likely inflated because key sources of measurement error are inadequately modeled and/or confounded with construct variance. We address these problems using three variations of multi-occasion bifactor models with Bayesian-derived parameter estimates to separate systematic variance into general and group factor effects and measurement error into three subcomponents (transient, specific-factor, and random-response). Collectively, these models produce indices of reliability and validity aligned with both standard confirmatory factor models and generalizability designs that extend interpretations of results to the broader domains from which items and occasions are sampled. We demonstrate how these techniques can provide new insights into psychometric properties of scores using Negative Emotionality domain and facet scales from the newly updated Big Five Inventory (BFI-2; Soto & John, 2017). Overall, the two-occasion congeneric bifactor model provided the best fit to the data and most informative indices for revising measures, examining dimensionality of composite and subscale scores, and evaluating the viability of those scores. We include code in R for analyzing all models in our extended online Supplemental Material. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
